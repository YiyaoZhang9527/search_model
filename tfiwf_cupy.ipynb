{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7cf31b92fcfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjieba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcut_for_search\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjieba_cut_for_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#from numpy import ndarray, array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mones\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0masnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cupy'"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# -*- encoding: utf-8 -*-\n",
    "'''\n",
    "@File    :   tfidf.py\n",
    "@Time    :   2020/10/05 22:09:15\n",
    "@Author  :   DataMagician \n",
    "@Version :   1.0\n",
    "@Contact :   408903228@qq.com\n",
    "'''\n",
    "\n",
    "# here put the import lib\n",
    "\n",
    "import numpy as np\n",
    "#from numpy import argwhere, zeros, ndarray, array, log, ones\n",
    "#from data_preprocessing_module import word_punct_tokenizer_for_chinese_function\n",
    "from tqdm import tqdm\n",
    "from jieba import cut as jiebacut\n",
    "from jieba import cut_for_search as jieba_cut_for_search\n",
    "#from numpy import ndarray, array\n",
    "from cupy import  zeros, ndarray, array, log, ones , ndarray , array , asnumpy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "need_type = list, tuple, ndarray, set\n",
    "\n",
    "'''加载标准停用词（标点符号）'''\n",
    "base_stopwords = ['.', '!', '?', '＂', '＃'\n",
    "    , '＄', '％', '＆', '＇', '（', '）', '＊'\n",
    "    , '＋', '，', '－', '／', '：', '；', '＜'\n",
    "    , '＝', '＞', '＠', '［', '＼', '］', '＾'\n",
    "    , '＿', '｀', '｛', '｜', '｝', '～', '｟'\n",
    "    , '｠', '｢', '｣', '､', '\\u3000', '、'\n",
    "    , '〃', '〈', '〉', '《', '》', '「', '」'\n",
    "    , '『', '』', '【', '】', '〔', '〕', '〖'\n",
    "    , '〗', '〘', '〙', '〚', '〛', '〜', '〝'\n",
    "    , '〞', '〟', '〰', '〾', '〿', '–', '—'\n",
    "    , '‘', '’', '‛', '“', '”', '„', '‟', '…'\n",
    "    , '‧', '﹏', '﹑', '﹔', '·', '.', '!'\n",
    "    , '?', '\"', '#', '$', '%', '&', \"'\", '('\n",
    "    , ')', '*', '+', ',', '-', '/', ':', ';'\n",
    "    , '<', '=', '>', '@', '[', '\\\\', ']', '^'\n",
    "    , '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "\n",
    "# TODO 中英文字符判断\n",
    "def is_chinese_function(uchar) -> chr:\n",
    "    '''\n",
    "     判断一个unicode是否是汉字\n",
    "    Args:\n",
    "        uchar: chart形式的字符\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_number_function(uchar) -> chr:\n",
    "    '''\n",
    "    判断一个unicode是否是数字\n",
    "    Args:\n",
    "        uchar:  chart形式的字符\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_alphabet_function(uchar) -> chr:\n",
    "    '''\n",
    "    判断一个unicode是否是英文字母\n",
    "    Args:\n",
    "        uchar: chart形式的字符\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if (uchar >= u'\\u0041' and uchar <= u'\\u005a') or (uchar >= u'\\u0061' and uchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_othe_function(uchar) -> chr:\n",
    "    '''\n",
    "    判断是否非汉字，数字和英文字符\n",
    "    Args:\n",
    "        uchar: chart形式的字符\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    if not (is_chinese_function(uchar) or is_number_function(uchar) or is_alphabet_function(uchar)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def character_type_token(original) -> str:\n",
    "    '''\n",
    "\n",
    "    Args: 字符串形式的文章\n",
    "        original:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "    不同字符类型分割\n",
    "    '''\n",
    "    make = [0]\n",
    "    diff = []\n",
    "    n = 0\n",
    "    temp = \"\"\n",
    "    for char in original:\n",
    "        if is_chinese_function(char):\n",
    "            n = 0\n",
    "        elif is_number_function(char):\n",
    "            n = 1\n",
    "        elif is_alphabet_function(char):\n",
    "            n = 2\n",
    "        elif is_othe_function(char):\n",
    "            n = 3\n",
    "        else:\n",
    "            n = 4\n",
    "        make.append(n)\n",
    "        if (make[-1] - make[-2]) == 0:\n",
    "            diff.append(char)\n",
    "        else:\n",
    "            diff.append(\"|\")\n",
    "            diff.append(char)\n",
    "    return \"\".join(diff).split(\"|\")\n",
    "\n",
    "\n",
    "# TODO 文章列表预处理函数\n",
    "def context_function(paper_list) -> (list, set, tuple):\n",
    "    '''\n",
    "    连接上下文本列表\n",
    "    Args: 文章列表\n",
    "        paper_list:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    return \"\".join(paper_list)\n",
    "\n",
    "\n",
    "def tokenize_chinese_function(original) -> str:\n",
    "    '''\n",
    "    中文分词\n",
    "    Args:\n",
    "        original: 一段文章字符串\n",
    "    Returns: 分词的列表\n",
    "    '''\n",
    "    tokens = []\n",
    "    for iter in jieba_cut_for_search(context_function(character_type_token(original))):\n",
    "        temp = ''\n",
    "        number = 0\n",
    "        iters = (iter.lower() if is_alphabet_function(iter) else iter)\n",
    "        n = 0\n",
    "        for char in iters:\n",
    "            char = str(ord(char))\n",
    "            n += 1 \n",
    "            temp+=char\n",
    "        tokens.append(int(temp))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def word_punct_tokenizer_for_chinese_function(article_list: list\n",
    "                                              , filter_stop_words=False) -> (list, tuple, ndarray, tuple, dict):\n",
    "    '''\n",
    "\n",
    "    Args: 对文章列表分词(中文优先)\n",
    "        article_list: 文章列表\n",
    "        filter_stop_words: 是否清理词不必要的停用词\n",
    "        True是过滤基础停用词，Flase是不过滤停用词，\n",
    "        如果是 list,tuple,dict,set,ndarray等可以\n",
    "        \"in\" 判断的结构则过滤定义的停用词\n",
    "\n",
    "    Returns:\n",
    "    '''\n",
    "    m = len(article_list)\n",
    "    if filter_stop_words == True:\n",
    "        return {paper_num: filter_stop_words_fumction(tokenize_chinese_function(paper)) for paper, paper_num in\n",
    "                zip(article_list, range(m))}\n",
    "    elif filter_stop_words == False:\n",
    "        return {paper_num: tokenize_chinese_function(paper) for paper, paper_num in zip(article_list, range(m))}\n",
    "    elif isinstance(filter_stop_words, (list, tuple, dict, ndarray, set)):\n",
    "        return {\n",
    "            paper_num: filter_stop_words_fumction(tokenize_chinese_function(paper), stop_words_dict=filter_stop_words)\n",
    "            for paper, paper_num in\n",
    "            zip(article_list, range(m))}\n",
    "\n",
    "\n",
    "def filter_stop_words_fumction(words_list: (list, ndarray)\n",
    "                               , stop_words_dict=base_stopwords) -> (list, tuple, set):\n",
    "    '''\n",
    "    过滤停用词\n",
    "    Args:\n",
    "        words_list: 需要过滤的词列表\n",
    "        stop_words_dict: 停用词表\n",
    "\n",
    "    Returns: 过滤停用词后的词列表\n",
    "\n",
    "    '''\n",
    "    return [word for word in words_list if word not in stop_words_dict]\n",
    "\n",
    "\n",
    "\n",
    "# TODO : 分步TF算法\n",
    "def tf_function(original_list: (need_type), word_vector) -> need_type:\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        paper_words: 文章列表\n",
    "        word_vector: 词汇表\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    m = len(original_list)\n",
    "    init_TF = zeros(m)\n",
    "    for word in original_list:\n",
    "        if word in word_vector:\n",
    "            index_ = argwhere(word_vector == word)[0][0]\n",
    "            init_TF[index_] += 1\n",
    "    return init_TF\n",
    "\n",
    "\n",
    "# TODO ：分步IDF算法\n",
    "def idf_function(paper_words_list, word_vector):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        paper_words_list: 文章列表\n",
    "        word_vector: 词汇表\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    m = word_vector.size\n",
    "    init_IDF = zeros(m)\n",
    "    N = paper_words_list.shape\n",
    "    n = -1\n",
    "    for word in word_vector:\n",
    "        n += 1\n",
    "        for paper_arr in paper_words_list:\n",
    "            if word in paper_arr:\n",
    "                init_IDF[n] += 1\n",
    "    return np.log(N / (init_IDF + 1))\n",
    "\n",
    "\n",
    "# TODO : 一次训练的整个训练TFIDF词向量\n",
    "def data_preprocessing_for_tfidf_function(original: (list, tuple, ndarray, set)\n",
    "                                          , filter_stop_words=True) -> (bool, list, tuple, ndarray, set):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        paper_words_list:\n",
    "        word_vector:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    word_punct_tokenizer = word_punct_tokenizer_for_chinese_function(original\n",
    "                                                                     , filter_stop_words=filter_stop_words)\n",
    "    vocabulary = []\n",
    "    for No, paper_tokens in word_punct_tokenizer.items():\n",
    "        vocabulary += paper_tokens\n",
    "    empty_words_dictionay = dict(zip(vocabulary, zeros(len(vocabulary))))\n",
    "    return {\"word_punct_tokenizer\": word_punct_tokenizer\n",
    "        , \"vocabulary_set\": set(vocabulary)\n",
    "        , \"empty_words_dictionay\": empty_words_dictionay}\n",
    "\n",
    "\n",
    "def TFIDF_function(original_list: (list, ndarray, set, tuple)\n",
    "                   , filter_stop_words=True):\n",
    "    '''\n",
    "    Args:\n",
    "        word_punct_tokens: 分词后的文章列表\n",
    "        vocabulary: 词汇表\n",
    "\n",
    "    Returns: tf 矩阵 横向量为词汇表\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    word_punct_tokenizer = data_preprocessing_for_tfidf_function(original_list, filter_stop_words=filter_stop_words)\n",
    "    word_punct_tokens, vocabulary = word_punct_tokenizer[\"word_punct_tokenizer\"] \\\n",
    "        , word_punct_tokenizer[\"vocabulary_set\"]\n",
    "    m, n = len(word_punct_tokens), len(vocabulary)\n",
    "    init_TF = zeros((m, n))\n",
    "    init_IDF = zeros(n)\n",
    "    init_counter_words_for_each_document = zeros(m)\n",
    "    \n",
    "    for No, paper_tokens in tqdm(word_punct_tokens.items(),\"tfidf训练\"):\n",
    "        vocabulary_of_each_document = len(paper_tokens)\n",
    "        init_TF += array([paper_tokens.count(word) for word in vocabulary])\n",
    "        init_IDF += array([word in paper_tokens and 1 or 0 for word in vocabulary])\n",
    "        init_counter_words_for_each_document[No] = vocabulary_of_each_document\n",
    "\n",
    "    TF = (init_TF.T / init_counter_words_for_each_document).T\n",
    "\n",
    "    IDF = log(m / (init_IDF + 1))\n",
    "\n",
    "    return {\"TF-IDF\": TF * IDF, \"TF\": TF, \"IDF\": IDF, \"init_TF\": init_TF[0], \"words_counter\": init_IDF.dot(ones(n)),\n",
    "            \"document_count\": m,\n",
    "            \"vocabulary_from_TF-IDF\": vocabulary}  # ,\"counter_vector\":init_IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_text = [[\"\"\"   职位职责：\n",
    "负责字节跳动多条产品线（抖音、今日头条、直播、游戏等）的风控安全研究及运营工作，持续和黑灰产斗智斗勇，打击作弊行为。\n",
    "1、情报监控体系建设：深入调查黑产市场，建立有效监控手段，持续进行作弊情报搜集与分析，为业务提供风险舆情预警；\n",
    "2、风险治理：分析案例进行策略优化，驱动产品安全机制完善与升级，并能量化风险拦截的价值；\n",
    "3、黑灰产行业研究：跟踪研究黑产发展变化趋势，输出分析调研报告。\"\"\"],\n",
    "\n",
    "[\"\"\"职位要求：\n",
    "1、对业务风险、数据有敏锐度，能够准确描述和识别作弊特征，擅长从大量信息中发现有价值的关键点；\n",
    "2、对业界主要的安全风控知识有一定了解，能够有效的收集开源情报信息，能够独立运用各种手段进行安全分析，并应用到实际产品中；\n",
    "3、拥有强烈的责任心和团队合作精神，出色的学习能力；\n",
    "4、具备强烈的好奇心和自我驱动力，喜欢接受挑战，追求**；\n",
    "5、具备风控、反作弊、反欺诈、安全及相关领域工作经验或熟悉黑产链条的运作模式或各类作弊手段的优先，有技术背景的优先。\"\"\"],\n",
    "[\"\"\"        职位职责：\n",
    "1、情报监控体系建设，深入调查黑产市场，建立有效监控手段，持续进行作弊情报搜集与分析，并输出对抗方案；\n",
    "2、黑产行业研究：跟踪研究黑产发展变化趋势，定期输出分析调研报告；\n",
    "职位要求：\n",
    "1、3年以上风控、反作弊、反欺诈、安全及相关领域工作经验，熟悉黑产链条的运作模式和各类作弊手段，有技术背景优先；\n",
    "2、对业界主要的安全技术有一定了解，能够有效的收集开源情报信息，能够独立运用各种手段进行安全分析，并应用到实际产品中；\n",
    "3、拥有强烈的责任心和团队合作精神，出色的学习能力；\n",
    "4、具备强烈的好奇心和自我驱动力，喜欢接受挑战，追求**。\"\"\"],[\"\"\"        岗位职责：\n",
    "1、负责分析用户反馈的内容，并根据反馈内容，给出相应的处理方案；\n",
    "2、负责用反缺陷原因分析，有效挖掘质量提升的点并推进解决.；\n",
    "3、负责对外包人员进行日常管理及工作指引\"\"\"],[\"\"\" 任职要求：\n",
    "1、1年以上地图行业工作经验，本科及以上学历，地图数据分析等经验者优先；\n",
    "、主动沟通，勤于思考，能够快速掌握处理用反问题的标准和方法，并根据实际工作，对标准和方法优化迭代，能快速掌握新的知识和技能；\n",
    "3、具备良好的数据分析能力，善于总结问题并能积极的进行过程优化；\n",
    "4、熟练使用office办公软件；\"\"\"],[\"\"\"工作职责:\n",
    "1、负责威胁情报生产的相关技术研发；\n",
    "2、进行网络威胁情报的收集，跟踪国内外的安全动态、威胁情报、安全漏洞等；\n",
    "3、阅读和翻译国外情报资料，负责情报分析研判，挖掘情报价值，提供及时的可行动的建议措施。\n",
    "岗位要求:\n",
    "1、熟悉威胁情报的获取、分析和挖掘，跟踪了解最新的攻击手法；\n",
    "2、能够独立进行各类安全日志分析与安全运营，并对各种安全事件的优先级和相应机制有比较深的理解，可以进行漏洞和**木马分析；\n",
    "3、有良好的文字能力和英文基础，熟练阅读英文网站文章；\n",
    "4、思路清晰，善于主动思考，有创新、能独立分析和解决问题，具有良好的沟通能力和团队合作精神；\n",
    "5、具备漏洞分析、**木马分析、Web攻防、威胁情报挖掘、反APT攻击相关工作经验优先\n",
    "6、有接触过SOC（安全运行中心）经验的优先；\n",
    "7、熟悉IBM、Splunk Soar平台的优先；\"\"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a>https://www.cnblogs.com/mokundong/p/tfiwf.html<a>\n",
    "\n",
    "    \n",
    "# 1, TFIDF\n",
    "## $ TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。TF-IDF $\n",
    "## $ 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF 分别代表了词频和逆向文档频率。$\n",
    "    \n",
    "## $ 词频 TF 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。$\n",
    "## $ 逆向文档频率 IDF，是指一个单词在文档中的区分度。$\n",
    "## $ 它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。$\n",
    "## $ IDF 越大就代表该单词的区分度越大。所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积. $\n",
    "\n",
    "## $ 这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，$\n",
    "\n",
    "## $ 即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。$\n",
    "\n",
    "## $ 但是TF-IDF在信息抽取上就不是很好的效果，因为tf-idf的idf在分母上采用了该单词的文档数，$\n",
    "## $ 所以就造成了文档一些关键词在特征表现上过于平滑 $\n",
    "\n",
    "# $$ 词频TF = 单词出现的次数/该文档的总单词数 $$\n",
    "# $$ 逆向文档的频率IDF = log(文档总数/该单词出现的文档数+1) $$\n",
    "# $$ 词频TF * 逆向文档的频率IDF $$\n",
    "\n",
    "    \n",
    "# 2, TF-IWF\n",
    "## 此处的 $T F$ 与 $T F-I D F$ 中意义一样，表示词频：\n",
    "\n",
    "# $$ t f_{i j}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} $$\n",
    "\n",
    "## 上式中分子 $n_{i, j}$ 表示词语 $t_{i}$ 在文本 $j$ 中的频数，分母 $\\sum_{k} n_{k, j}$ 表示文档 $j$ 中所有词汇量总和，即是说：\n",
    "    \n",
    "# $$ T F_{w}=\\frac{\\text { 给定词 } w \\text { 在当前文章出现的次数 }}{\\text { 当前文章中的总词量 }} $$\n",
    "    \n",
    "## $ 不同之处在于 $I W F$ 部分，定义为：$\n",
    "\n",
    "# $$ i w f_{i}=\\log \\frac{\\sum_{i=1}^{m} n t_{i}}{n t_{i}} $$\n",
    "\n",
    "## 上式中分子 $\\sum_{i=1}^{m} n t_{i}$ 表示语料库中所有词语的频数之和，分母 $n t_{i}$ 表示词语 $t_{i}$ 在语料库中的总频数，即：\n",
    "    \n",
    "# $$ I W F_{i}=\\frac{\\text { 语料库中所有词语的频数 }}{\\text { 给定词 } w \\text { 在语料库中出现的频数和 }} $$\n",
    "    \n",
    "## 因此, $T F-I W F$ 定义为：\n",
    "\n",
    "# $$ T F-I W F_{i, j} \\rightarrow t f_{i, j} \\times i w f_{i}=\\frac{n_{i, j}}{\\sum_{k} n_{k, j}} \\times \\log \\frac{\\sum_{i=1}^{m} n t_{i}}{n t_{i}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIWF_function(original_list: (list, ndarray, set, tuple)\n",
    "                   , filter_stop_words=True):\n",
    "    '''\n",
    "    Args:\n",
    "        word_punct_tokens: 分词后的文章列表\n",
    "        vocabulary: 词汇表\n",
    "\n",
    "    Returns: tf 矩阵 横向量为词汇表\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "    word_punct_tokenizer = data_preprocessing_for_tfidf_function(original_list, filter_stop_words=filter_stop_words)\n",
    "    word_punct_tokens, vocabulary = word_punct_tokenizer[\"word_punct_tokenizer\"] , word_punct_tokenizer[\"vocabulary_set\"]\n",
    "    m, nti = len(word_punct_tokens), len(vocabulary)\n",
    "    init_TF = zeros((m, nti))\n",
    "    init_IWF = zeros(nti)\n",
    "    init_counter_words_for_each_document = zeros(m)\n",
    "    \n",
    "    for No, paper_tokens in tqdm(word_punct_tokens.items(),\"tfidf训练\"):\n",
    "        vocabulary_of_each_document = len(paper_tokens)\n",
    "        init_TF += array([paper_tokens.count(word) for word in vocabulary])\n",
    "        init_IWF += array([word in paper_tokens and 1 or 0 for word in vocabulary])\n",
    "        init_counter_words_for_each_document[No] = vocabulary_of_each_document\n",
    "        #print(init_IWF)\n",
    "\n",
    "    TF = (init_TF.T / init_counter_words_for_each_document).T\n",
    "\n",
    "    IWF = log(nti / (init_IWF))\n",
    "\n",
    "    return {\"TF-IWF\": TF * IWF, \"TF\": TF, \"IDF\": IWF, \"init_TF\": init_TF[0],\"document_count\": m,\"vocabulary_from_TF-IDF\": vocabulary ,\"counter_vector\":init_IWF }\n",
    "            #, \"words_counter\": init_IwF.dot(ones(nti)),\n",
    "              # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "TFIWF = TFIWF_function(case_text)\n",
    "plt.figure(figsize=(26,9))\n",
    "for vec in asnumpy(TFIWF[\"TF-IWF\"]):\n",
    "    plt.plot(vec)\n",
    "#plt.xlabel([str(i) for i in TFIWF[\"vocabulary_from_TF-IDF\"]])\n",
    "#plt.ylabel(range(len(TFIWF[\"vocabulary_from_TF-IDF\"])))\n",
    "plt.title(\"TF-IDF\",c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对TF-IDF效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(26,9))\n",
    "for vec in asnumpy(TFIDF_function(case_text)[\"TF-IDF\"]):\n",
    "    plt.plot(vec)\n",
    "plt.title(\"TF-IDF\",c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}